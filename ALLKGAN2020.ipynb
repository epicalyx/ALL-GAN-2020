{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALLKGAN2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nsx2WMTeKoc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Introduction\n",
        "\n",
        "#### - This notebook explores a novel convolutional network architechture as discussed in the following research paper to build a classification system for better assistance in diagonosing Acute Lymphoblastic Leukemia in blood cells.\n",
        "**[Research Paper](http://www.ijcte.org/vol10/1198-H0012.pdf)**\n",
        "\n",
        "\n",
        "#### - The dataset has been taken from : [Link](https://homes.di.unimi.it/scotti/all/)\n",
        "\n",
        "* Here, ALL_IDB2 version of the dataset has been used\n",
        "\n",
        "* This dataset is completely balanced with equal number of samples in both the classes.\n",
        "\n",
        "\n",
        "#### - Data augmentation ensures that data is large enough and model extracts features efficiently without overfitting and therefore we have analysed two types of data augmentation techniques in this notebook\n",
        "* A particular type of GAN called [SinGAN](https://arxiv.org/pdf/1905.01164.pdf) was used alongwith the following techniques mentioned in the research paper:\n",
        "\n",
        "   1. Grayscaling of image\n",
        "   2. Horizontal reflection\n",
        "   3. Vertical reflection\n",
        "   4. Gaussian Blurring\n",
        "   5. Histogram Equalization\n",
        "   6. Rotation\n",
        "   7. Translation\n",
        "   8. Shearing\n",
        "\n",
        "* SinGAN without the above techniques\n",
        "\n",
        "**The dataset was split into 80% and 20% for training and testing respectively.**\n",
        "\n",
        "#### - The details of methodologies and results of our present analysis is present [here](https://docs.google.com/document/d/11XXjFRofXlyNGcE_plRDMO4xjxELFnkVqGtBEGomL2k/edit?usp=sharing)\n",
        "#### - It is also worth noting the biases present in our methodology, ethical concerns and qualititaive interpretation of our results, mentioned in the doc\n",
        "\n",
        "\n",
        "\n",
        "#### Below is the detailed code implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfZJS4lHhUXE",
        "colab_type": "text"
      },
      "source": [
        "### Loading requires packages\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbOH9uy7is9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ8BE0xmnLD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import random\n",
        "import cv2\n",
        "from numpy.random import seed\n",
        "from tensorflow import set_random_seed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy import ndimage\n",
        "from skimage import exposure\n",
        "import skimage\n",
        "from skimage import io\n",
        "from skimage import transform as tm\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow.keras\n",
        "from keras.utils import np_utils\n",
        "import keras_metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix,precision_score,recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH2z2eQB6ZpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for consistemt results across multiple executions\n",
        "seed(3)\n",
        "set_random_seed(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXixiXFmnVVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tensorflow.keras.__version__)\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja9psxD3owgj",
        "colab_type": "text"
      },
      "source": [
        "## **Mount your Google Drive**\n",
        "\n",
        " \n",
        "\n",
        "##### Upload the **ALL-Keras-2019** directory from your cloned repo to the root of your Google Drive. Use the following commands and follow the provided steps to mount your Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqiLXt66iRb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_dir = \"/content/drive/My Drive/ALL-Keras-2019/\"\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFUHmCQhpAQK",
        "colab_type": "text"
      },
      "source": [
        "#### **You will notice the data folder in the Model directory, Model/Data, inside you have Train and Test.**\n",
        "\n",
        "#### **You can place all the images inside the *Train* folder. We will split them into training and test set below**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OtM4ERApGY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'Model/Data/Train'\n",
        "dataset = Path(root_dir + data_dir)\n",
        "images= dataset.glob(\"*.tif\")\n",
        "data = []\n",
        "\n",
        "for img in images:\n",
        "  name, ext = os.path.splitext(os.path.basename(img))\n",
        "  if name[-1]=='1':\n",
        "    data.append((img,1))\n",
        "  elif name[-1]=='0':\n",
        "    data.append((img,0))\n",
        "    \n",
        "data_frame = pd.DataFrame(data,columns=['image','label'],index = None)\n",
        "data_frame = data_frame.sample(frac=1.).reset_index(drop=True)\n",
        "data_frame.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJjM0x57pN94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Splitting training and test data; we will not be augmenting test data\n",
        "orig_train = pd.DataFrame()\n",
        "test = pd.DataFrame()\n",
        "\n",
        "orig_train = data_frame[:130]\n",
        "test_data = data_frame[130:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXaa_96_2AJU",
        "colab_type": "text"
      },
      "source": [
        "##**Augmentation Techniques**\n",
        "\n",
        "**Note: Test data should never be augmented and so we will only augment training set**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmopgUB4pp8B",
        "colab_type": "text"
      },
      "source": [
        "###**1. Using [SinGAN](https://arxiv.org/pdf/1905.01164.pdf)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aweicqXMpUJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/tamarott/SinGAN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT2nNIZ3soYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SinGAN works on a single image. We will choose 13 original images from the \n",
        "# training set randomly and generate 50 artificial images from each. \n",
        "\n",
        "random_img = []\n",
        "for i in range(13):\n",
        "  n = random.randint(0,129)\n",
        "  random_img.append(n)\n",
        "print(random_img)\n",
        "\n",
        "aug_list = []\n",
        "\n",
        "def augment(ind):\n",
        "  %cd /content/SinGAN\n",
        "  !python main_train.py --input_name train['image'][ind]\n",
        "  !python random_samples.py --input_name train['image'][ind] --mode random_samples --gen_start_scale 9\n",
        "  %cd /content/SinGAN/Output/RandomSamples/train['image'][ind]/gen_start_scale=9\n",
        "  for i in range(50):\n",
        "    aug_list.append((str(i)+'.png',train['label'][ind]))\n",
        "\n",
        "# This will start generating 50 images for each randomly chosen training image. \n",
        "for i in range(13):\n",
        "  augment[random_img[i]]\n",
        "\n",
        "aug_train = pd.DataFrame(aug_list,columns=['image','label'],index = None)\n",
        "aug_train = aug_df.sample(frac=1.).reset_index(drop=True)\n",
        "\n",
        "# We will now combine original and augmented images into a single dataframe\n",
        "train_data = pd.concat([orig_train,aug_train]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zewhaj6Ul0Y_",
        "colab_type": "text"
      },
      "source": [
        "### **2. Augmentation as presented in the [paper](http://www.ijcte.org/vol10/1198-H0012.pdf)**\n",
        "\n",
        "### 8 augmentation techniques have been used here\n",
        "1. Grayscaling of image\n",
        "2. Horizontal reflection \n",
        "3. Vertical reflection\n",
        "4. Gaussian Blurring \n",
        "5. Histogram Equalization\n",
        "6. Rotation\n",
        "7. Translation\n",
        "8. Shearing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f71MR6OdWyJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# histogram equalization function\n",
        "def hist(img):\n",
        "  img_to_yuv = cv2.cvtColor(img,cv2.COLOR_BGR2YUV)\n",
        "  img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0])\n",
        "  hist_equalization_result = cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2BGR)\n",
        "  return hist_equalization_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81rPbbtegU2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to perform rotation on an image\n",
        "def rotation(img):\n",
        "  rows,cols = img.shape[0],img.shape[1]\n",
        "  randDeg = random.randint(-180, 180)\n",
        "  matrix = cv2.getRotationMatrix2D((cols/2, rows/2), randDeg, 0.70)\n",
        "  rotated = cv2.warpAffine(img, matrix, (rows, cols), borderMode=cv2.BORDER_CONSTANT,\n",
        "                                     borderValue=(144, 159, 162))\n",
        "  return rotated     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urz26j6qZJFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to perform shearing of an image\n",
        "def shear(img):\n",
        "  # Create Afine transform\n",
        "  afine_tf = tm.AffineTransform(shear=0.5)\n",
        "  # Apply transform to image data\n",
        "  modified = tm.warp(img, inverse_map=afine_tf)\n",
        "  return modified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYS6DhKxKrvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aug_method(dataframe,dim,aug=True):\n",
        "  if aug:\n",
        "    n = len(dataframe)\n",
        "    data = np.zeros((n*6,dim,dim,3),dtype = np.float32)\n",
        "    labels = np.zeros((n*6,2),dtype = np.float32)\n",
        "    count = 0\n",
        "\n",
        "    for j in range(0,n):\n",
        "      img_name = dataframe.iloc[j]['image']\n",
        "      label = dataframe.iloc[j]['label']\n",
        "      encoded_label = np_utils.to_categorical(label, num_classes=2)\n",
        "      img = cv2.imread(str(img_name))\n",
        "      img = cv2.resize(img, (dim,dim))\n",
        "\n",
        "      if img.shape[2]==1:\n",
        "        img = np.dstack([img, img, img])\n",
        "      orig_img = img.astype(np.float32)/255.\n",
        "      data[count] = orig_img\n",
        "      labels[count] = encoded_label\n",
        "      # Cases where we also use SinGAN as a data augmentation technique, only 5 out of the above 8 have been used.      \n",
        "      aug_img1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "      aug_img2 = cv2.flip(img, 0) \n",
        "      #aug_img3 = cv2.flip(img,1)\n",
        "      #aug_img4 = ndimage.gaussian_filter(img, sigma= 5.11)\n",
        "      aug_img5 = hist(img)\n",
        "      aug_img6 = rotation(img)\n",
        "      aug_img7 = cv2.warpAffine(img, np.float32([[1, 0, 84], [0, 1, 56]]), (img.shape[0], img.shape[1]),\n",
        "                                  borderMode=cv2.BORDER_CONSTANT, borderValue=(144, 159, 162))\n",
        "      #aug_img8 = shear(img)\n",
        "      aug_img1 = np.dstack([aug_img1, aug_img1, aug_img1])\n",
        "\n",
        "      aug_img1 = aug_img1.astype(np.float32)/255.                 \n",
        "      aug_img2 = aug_img2.astype(np.float32)/255.\n",
        "      #aug_img3 = aug_img3.astype(np.float32)/255. \n",
        "      #aug_img4 = aug_img4.astype(np.float32)/255.\n",
        "      aug_img5 = aug_img5.astype(np.float32)/255.\n",
        "      aug_img6 = aug_img6.astype(np.float32)/255.\n",
        "      aug_img7 = aug_img7.astype(np.float32)/255.\n",
        "      #aug_img8 = aug_img8.astype(np.float32)/255.\n",
        "\n",
        "      data[count+1] = aug_img1\n",
        "      labels[count+1] = encoded_label\n",
        "      data[count+2] = aug_img2\n",
        "      labels[count+2] = encoded_label\n",
        "      data[count+3] = aug_img5\n",
        "      labels[count+3] = encoded_label\n",
        "      data[count+4] = aug_img6\n",
        "      labels[count+4] = encoded_label\n",
        "      data[count+5] = aug_img7\n",
        "      labels[count+5] = encoded_label\n",
        "      #data[count+6] = aug_img5\n",
        "      #labels[count+6] = encoded_label\n",
        "      #data[count+7] = aug_img5\n",
        "      #labels[count+7] = encoded_label\n",
        "      #data[count+8] = aug_img5\n",
        "      #labels[count+8] = encoded_label\n",
        "      count +=6      \n",
        "  else:\n",
        "    n = len(dataframe) \n",
        "    data = np.zeros((n,dim,dim,3),dtype = np.float32)\n",
        "    labels = np.zeros((n,2),dtype = np.float32) \n",
        "    count = 0\n",
        "    for j in range(0,n):   \n",
        "      img_name = dataframe.iloc[j]['image']\n",
        "      label = dataframe.iloc[j]['label']      \n",
        "      encoded_label = np_utils.to_categorical(label, num_classes=2)            \n",
        "      img = cv2.imread(str(img_name))\n",
        "      img = cv2.resize(img, (dim,dim))      \n",
        "      if img.shape[2]==1:    \n",
        "        img = np.dstack([img, img, img])                                    \n",
        "      orig_img = img.astype(np.float32)/255.                       \n",
        "      data[count] = orig_img\n",
        "      labels[count] = encoded_label    \n",
        "      count +=1                      \n",
        "  return data,labels                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LTeS8gRRuXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aug_mode(mode):\n",
        "  if mode=='both':\n",
        "    X_train,y_train = aug_method(train_data,dim=100,aug=True)\n",
        "    X_test,y_test = aug_method(test_data,dim=100,aug=False)\n",
        "  elif mode=='SinGAN':\n",
        "    X_train,y_train = aug_method(train_data,dim=100,aug=False)\n",
        "    X_test,y_test = aug_method(test_data,dim=100,aug=False)\n",
        "\n",
        "  X_train = np.asarray(X_train)\n",
        "  y_train = np.asarray(y_train)\n",
        "  X_test = np.asarray(X_test)\n",
        "  y_test = np.asarray(y_test)\n",
        "  print('Shape of training data:',X_train.shape)\n",
        "  print('Shape of test data:',X_test.shape)\n",
        "\n",
        "# augmentation with both SinGAN and other techniques\n",
        "aug_mode('both')\n",
        "\n",
        "# augmentation only with SinGAN\n",
        "aug_mode('SinGAN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrAQNIXMpPv7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "###**The following model was used in the paper**\n",
        "Additionaly three dropout layers with different dropout rates have been used to reduce overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6-Em20CpBof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(16,(5,5),padding='valid',input_shape = X_train.shape[1:]))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32,(5,5),padding='valid'))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\n",
        "model.add(tf.keras.layers.Dropout(0.6))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64,(5,5),padding='valid'))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.8))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(2,activation = 'softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnjtIKJqt28j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6c9jTb_L5YU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model visualization\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzEj3RvpHYNA",
        "colab_type": "text"
      },
      "source": [
        "#### **Case 1: Using both data augmentation methods**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p76OFKBR7Z1Q",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "epochs = 50\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.0001, decay = 1e-6)\n",
        "model.compile(loss = 'binary_crossentropy',optimizer = optimizer, metrics = ['accuracy',keras_metrics.precision(), keras_metrics.recall()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ3_r2427ZNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train,y_train,steps_per_epoch = int(len(X_train)/batch_size),epochs=epochs)\n",
        "history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OxCE-c3aOlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test,y_test,verbose=0)\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jyrtwMoaYe2",
        "colab": {}
      },
      "source": [
        "# Accuracy and loss curves\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDVEOtfNTkP8",
        "colab_type": "text"
      },
      "source": [
        "#### **Case 2: Using only SinGAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "24JH9nS5ZWnr",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 50\n",
        "optimizer = tf.keras.optimizers.rmsprop(lr = 0.0001, decay = 1e-6)\n",
        "model.compile(loss = 'binary_crossentropy',optimizer = optimizer, metrics = ['accuracy',keras_metrics.precision(), keras_metrics.recall()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p7goEChZmvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train,y_train,steps_per_epoch = int(len(X_train)/batch_size),epochs=epochs)\n",
        "history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAyuNjNDqKwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test,y_test,verbose=0)\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Coml4pvkqZHP",
        "colab": {}
      },
      "source": [
        "# Accuracy and loss plots\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}